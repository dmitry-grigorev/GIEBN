{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общий код между ноутбуками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output \n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дискретизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(a):\n",
    "    vc = a.value_counts()\n",
    "    vc/=vc.sum()\n",
    "    return -(vc*np.log(vc)).sum()\n",
    "\n",
    "def inconfidence_score(y_true, y_pred):\n",
    "    return np.mean((1-y_pred)*y_true + y_pred*(1-y_true))\n",
    "\n",
    "def discretize(data, variables, icat, icont, contdiscstrategy=\"kmeans\", n_bins=3,\n",
    "               automatic_fix=True\n",
    "               \n",
    "               ):\n",
    "    transformers_data = dict()\n",
    "    cont_features = [variables[i] for i in icont]\n",
    "    \n",
    "    if icat is None: # в датасете только непрерывные фичи (и предиктор)\n",
    "        pipeline = make_pipeline(\n",
    "            KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", random_state=42, strategy=contdiscstrategy), \n",
    "            FunctionTransformer(lambda x: x.astype(\"int\")))\n",
    "    elif icont is None: # в датасете только категориальные фичи (и предиктор)\n",
    "        pipeline = make_pipeline(\n",
    "            OrdinalEncoder(categories=\"auto\"), \n",
    "            FunctionTransformer(lambda x: x.astype(\"int\")))\n",
    "    else:\n",
    "        pipeline = make_pipeline(make_union(\n",
    "        make_pipeline(FunctionTransformer(lambda x: x.iloc[:, icat]), OrdinalEncoder(categories=\"auto\")),\n",
    "        make_pipeline(FunctionTransformer(lambda x: x.loc[:, icont]), KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", random_state=42, strategy=contdiscstrategy))\n",
    "    ),\n",
    "        FunctionTransformer(lambda x: x.astype(\"int\")))\n",
    "    \n",
    "    \n",
    "    pipeline.fit(data)\n",
    "    encoded_data = pd.DataFrame(pipeline.transform(data), columns=data.columns if icat is None or icont is None\\\n",
    "                                                                                        else data.columns[icat+icont])\n",
    "    encoded_data = encoded_data[variables] # в общем случае пайплайн переставляет признаки, возвращаем их на их места здесь\n",
    "\n",
    "    warning_splits_features = []\n",
    "\n",
    "    for feat in encoded_data.columns:\n",
    "        if entropy(encoded_data[feat]) < 0.85 and feat in cont_features:\n",
    "            warning_splits_features.append(feat)\n",
    "            print(f\"Warning: feature {feat} has practically degenerate states and low entropy\")\n",
    "\n",
    "    if automatic_fix:\n",
    "        q = np.linspace(0, 1, 11)\n",
    "\n",
    "\n",
    "\n",
    "        dicval = {feat: [data[feat].quantile(qu) for qu in q] for feat in cont_features}\n",
    "        if icat is None:\n",
    "            v = pipeline.steps[0][1].bin_edges_\n",
    "            f = list(pipeline.steps[0][1].get_feature_names_out())\n",
    "        elif icat is not None and icont is not None:\n",
    "            v = pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].bin_edges_\n",
    "            f = list(pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].get_feature_names_out())\n",
    "        indexes_out = [i for i, feat in enumerate(f) if feat in warning_splits_features]\n",
    "\n",
    "\n",
    "        for i in indexes_out:\n",
    "            v[i] = [v[i][0], dicval[f[i]][3], dicval[f[i]][7], v[i][3]]\n",
    "        \n",
    "        if icat is None:\n",
    "            pipeline.steps[0][1].bin_edges_ = v\n",
    "        elif icat is not None and icont is not None:\n",
    "            pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].bin_edges_ = v\n",
    "        pipeline.steps[0][1].bin_edges_ = v\n",
    "        encoded_data = pd.DataFrame(pipeline.transform(data), columns=data.columns if icat is None or icont is None\\\n",
    "                                                                                        else data.columns[icat+icont])\n",
    "\n",
    "    transformers_data[\"transformer\"] = pipeline\n",
    "    \n",
    "    if icat is None:\n",
    "        transformers_data[\"cont_features\"]       = list(pipeline.steps[0][1].get_feature_names_out())\n",
    "        transformers_data[\"cont_features_edges\"] = pipeline.steps[0][1].bin_edges_\n",
    "        \n",
    "    elif icont is None:\n",
    "        transformers_data[\"cat_features\"]            = list(pipeline.steps[0][1].get_feature_names_out())\n",
    "        transformers_data[\"cat_features_categories\"] = pipeline.steps[0][1].categories_\n",
    "        \n",
    "    else:\n",
    "        transformers_data[\"cont_features\"]           = list(pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].get_feature_names_out())\n",
    "        transformers_data[\"cont_features_edges\"]     = pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].bin_edges_\n",
    "        transformers_data[\"cat_features\"]            = list(pipeline.steps[0][1].named_transformers[\"pipeline-1\"].steps[1][1].get_feature_names_out())\n",
    "        transformers_data[\"cat_features_categories\"] = pipeline.steps[0][1].named_transformers[\"pipeline-1\"].steps[1][1].categories_\n",
    "    \n",
    "    #kmeanspipeline.steps[0][1].named_transformers[\"pipeline-1\"].steps[1][1].categories_[0]\n",
    "    return encoded_data, transformers_data\n",
    "\n",
    "#disc_data, pipeline_data = discretize(data, variables, icat=None, icont=[0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка таблиц усл. вероятностей по БС с градациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_cpds(bn_info, distributions, n_states_map): # moved to GIEBN\n",
    "    cpds = list()\n",
    "    for index, row in bn_info.iterrows():\n",
    "        feat = row[\"name\"].name\n",
    "        if len(row[\"parents\"]) == 0:\n",
    "            # cpd is just a pd\n",
    "            cpd = TabularCPD(feat, n_states_map[feat], [[e] for e in distributions[feat][\"cprob\"]])\n",
    "            cpds.append(cpd)\n",
    "        else:\n",
    "            cpd_list = [probs for probs in distributions[feat][\"cprob\"].values()]\n",
    "            #cpd_list = [probs for i, probs in distributions[feat][\"cprob\"].items() if i[0]!=\"[\"]\n",
    "            #print(cpd_list)\n",
    "            nrows = len(cpd_list)\n",
    "            ncols = len(cpd_list[0])\n",
    "            cpd_list = [[cpd_list[i][j] for i in range(nrows)] for j in range(ncols)]\n",
    "            #print(feat, row[\"parents\"])\n",
    "            #print(cpd_list)\n",
    "            cpd = TabularCPD(feat, n_states_map[feat], cpd_list, evidence=row[\"parents\"], evidence_card=[n_states_map[p] for p in row[\"parents\"]])\n",
    "            cpds.append(cpd)\n",
    "    return cpds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(data,\n",
    "                       d_dict,# словарь списков фактор-градация\n",
    "                       b_sample_size,\n",
    "                       metrics_list, trials=1000, alpha=0, incl_random_removal=False,\n",
    "                       mode='regr',\n",
    "                       drop_mode='random',\n",
    "                       reference='bn',\n",
    "                       incl_test=False,\n",
    "                       test_size=0.3,\n",
    "                       stratify_tts=True):\n",
    "    \n",
    "    n_ref=np.inf\n",
    "\n",
    "    metrics_results={\n",
    "        k: [list() for _ in range(len(metrics_list))] for k in d_dict.keys()\n",
    "    }\n",
    "    metrics_results['init'] = [list() for _ in range(len(metrics_list))]\n",
    "\n",
    "    \n",
    "\n",
    "    if incl_random_removal:\n",
    "        metrics_results['random'] = [list() for _ in range(len(metrics_list))]\n",
    "\n",
    "    metrics_results_test = {k: [list() for _ in range(len(metrics_list))] for k in metrics_results.keys()}\n",
    "\n",
    "    n_dropped_stats={k: list() for k in metrics_results.keys()}\n",
    "\n",
    "    #пробуем побутсрапировать выборку, чтобы оценить значимость различий в ошибках регрессии\n",
    "    for i in tqdm(range(trials)):\n",
    "        r_seed = 42+i\n",
    "        np.random.seed(r_seed)\n",
    "        #indexes = np.random.choice(data.index, size=b_sample_size)\n",
    "        #bsample = data.loc[indexes].reset_index()[data.columns]\n",
    "        if not incl_test:\n",
    "            bsample = data.sample(n=b_sample_size, replace=False).reset_index()[data.columns]\n",
    "        else:\n",
    "            bsample_train, b_sample_test = train_test_split(data, random_state=r_seed,\n",
    "                                                            test_size=test_size,\n",
    "                                                            stratify=data[target] if stratify_tts else None)\n",
    "            bsample = bsample_train.sample(n=b_sample_size, replace=True).reset_index()[data.columns]\n",
    "        \n",
    "        model_base = copy(model)\n",
    "        model_base.fit(bsample[features], bsample[target])\n",
    "\n",
    "        if mode=='regr':\n",
    "            y_pred = model_base.predict(bsample[features]) # depends on task (regr/classif)\n",
    "        else:\n",
    "            y_pred = model_base.predict_proba(bsample[features])[:, 1] # depends on task (regr/classif)\n",
    "\n",
    "        data_errors = bsample.copy(deep=True)\n",
    "        if mode=='regr':\n",
    "            data_errors[\"ape_error\"] = np.abs((bsample[target]-y_pred)/bsample[target])\n",
    "        else:\n",
    "            #inconf scode\n",
    "            data_errors[\"inconf_error\"]=(1-y_pred)*bsample[target]+(y_pred)*(1-bsample[target])\n",
    "\n",
    "\n",
    "        data_errors.drop(columns=[target], inplace=True)\n",
    "\n",
    "        bsample_disc = pd.DataFrame(pipeline_data[\"transformer\"].transform(data_errors), columns=data_errors.columns)\n",
    "        \n",
    "        bsample_disc = bsample_disc.reindex(index=data_errors.index)\n",
    "        \n",
    "        #print(pipeline_data[\"transformer\"].transform(data_errors).shape[0])\n",
    "        iterlist = [reference]+[x for x in metrics_results.keys() if x != reference]\n",
    "\n",
    "        for g in iterlist:\n",
    "            mask = False\n",
    "            if g == 'init':\n",
    "                mask = None\n",
    "                samp = bsample\n",
    "            elif g == 'random':\n",
    "                mask=None\n",
    "                indexes_to_drop = np.random.choice(bsample.index, size=n_ref, replace=False)\n",
    "                samp = bsample.drop(index=indexes_to_drop)\n",
    "            else:\n",
    "                mask=False\n",
    "                for feat, cat in zip(d_dict[g][0], d_dict[g][1]):\n",
    "                    mask = mask | (bsample_disc[feat]==cat)\n",
    "                \n",
    "                if drop_mode == 'random':\n",
    "                    indexes_to_drop = np.random.choice(bsample[mask].index, size=int(np.floor(bsample[mask].shape[0]*(1-alpha))), replace=False)\n",
    "                elif drop_mode == 'metric':\n",
    "                    if mode=='regr':\n",
    "                        errors = data_errors[mask]['ape_error']\n",
    "                    else:\n",
    "                        #inconf scode\n",
    "                        #print(g)\n",
    "                        #print(mask.index, data_errors.index)\n",
    "                        #print(bsample.shape[0], bsample_disc.shape[0], data_errors.shape[0], mask.shape, indexes.shape[0])\n",
    "                        errors = data_errors[mask][\"inconf_error\"]\n",
    "\n",
    "                    errors_sorted = errors.sort_values(ascending=False)\n",
    "                    size = min(n_ref, int(np.floor(errors.shape[0]*(1-alpha))))\n",
    "\n",
    "                    # т.к. подходы могут отбрасывать разное кол-во наблюдений, получаемый результат по метрикам будет разным из-за этого\n",
    "\n",
    "                    #thresh = errors_sorted[:size].min()\n",
    "                    #indexes_to_drop = errors[errors>=thresh].index\n",
    "                    thresh = errors_sorted[-size:].max()\n",
    "                    indexes_to_drop = errors[errors<thresh].index\n",
    "                \n",
    "                samp = bsample.drop(index=indexes_to_drop)\n",
    "                if g==reference:\n",
    "                    n_ref=indexes_to_drop.shape[0]\n",
    "                    \n",
    "                n_dropped_stats[g].append(indexes_to_drop.shape[0])\n",
    "        # --\n",
    "            \n",
    "            X1, y1 = samp[features], samp[target]\n",
    "\n",
    "            model1 = copy(model)\n",
    "            model1.fit(X1, y1)  \n",
    "\n",
    "            def calculate_metrics(model, X_data, y_data):\n",
    "                dict_m = {}\n",
    "                if mode=='regr':\n",
    "                    y1_pred = model.predict(X_data) # depends on task (regr/classif)\n",
    "                else:\n",
    "                    y1_pred = model.predict_proba(X_data)[:, 1] # depends on task (regr/classif)\n",
    "\n",
    "                for k, metric in enumerate(metrics_list):\n",
    "                    dict_m[k] = metric(y_data, y1_pred)\n",
    "\n",
    "                return dict_m\n",
    "\n",
    "\n",
    "\n",
    "            train_res = calculate_metrics(model1, X1, y1)\n",
    "            for k, metric in enumerate(metrics_list):\n",
    "                metrics_results[g][k].append(train_res[k])\n",
    "\n",
    "            if incl_test:\n",
    "                test_res = calculate_metrics(model1, b_sample_test[features], b_sample_test[target])\n",
    "                for k, metric in enumerate(metrics_list):\n",
    "                    metrics_results_test[g][k].append(test_res[k])\n",
    "\n",
    "    return_value = {'metrics': metrics_results,\n",
    "             'n_dropped': n_dropped_stats,\n",
    "             'avg_n_dropped': {k: sum(v)/len(v) if len(v) >0 else -1 for k, v in n_dropped_stats.items()}}\n",
    "    \n",
    "    if incl_test:\n",
    "        return_value['metrics_test'] = metrics_results_test\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(data):\n",
    "    return [f\"{feat}_{int(k)}\" for feat in data.columns for k in\n",
    "            sorted(data[feat].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbn_training_pipeline(gbn, disc_data_encoded, error_name, categories, score=BicScore):\n",
    "    gbn.add_nodes({\"types\": {feat: \"disc\" for feat in disc_data_encoded.columns}})\n",
    "    features_cats = [x for x in disc_data_encoded.columns if error_name not in x]\n",
    "    error_cats = [x for x in disc_data_encoded.columns if error_name in x]\n",
    "\n",
    "    blacklist = [(x, y) for x, y in product(error_cats, features_cats)] \\\n",
    "            + [(x, y) for x, y in product(categories, categories) if x.split(\"_\")[:-1]==y.split(\"_\")[:-1]]\n",
    "\n",
    "    gbn.add_edges(disc_data_encoded, scoring_function=(\"K2\", score), params={\"bl_add\": blacklist})\n",
    "    gbn.fit_parameters(data=disc_data_encoded, n_jobs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_error(data, model, features, target, error_col_name='inconf_error', task='classif'):\n",
    "    data_errors = data.copy(deep=True)\n",
    "    if task=='classif':\n",
    "        probs = model.predict_proba(data[features])[:, 1]\n",
    "        data_errors[error_col_name] = (1-probs)*data[target]+(probs)*(1-data[target]) #inconfidence measure: the larger the worser inconfidence\n",
    "    \n",
    "\n",
    "    elif task=='regr':\n",
    "        data_errors[error_col_name] = np.abs((data[target]-model.predict(data[features]))/data[target])\n",
    "    data_errors.drop(columns=[target], inplace=True)\n",
    "    return data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_predecessors(gbn, nodes):\n",
    "    res = []\n",
    "    iter_list=nodes\n",
    "    while len(iter_list)>0:\n",
    "        node = iter_list[0]\n",
    "        iter_list=iter_list[1:]\n",
    "        if gbn[node] is not None:\n",
    "            pred = gbn[node].disc_parents\n",
    "            if pred is not None:\n",
    "                for p in pred:\n",
    "                    if p not in res and p not in nodes:\n",
    "                        res.append(p)\n",
    "                        iter_list.append(p)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_prob(gbn, node, nodes_values):\n",
    "    if len(gbn[node].disc_parents)==0:\n",
    "        return gbn.get_dist(node)['cprob'][int(nodes_values[node])]\n",
    "    else:\n",
    "        return gbn.get_dist(node, nodes_values)[int(nodes_values[node])]\n",
    "\n",
    "\n",
    "def calculate_cond_prob_support(gbn, e_grad, x_grad):\n",
    "    pgmpy_gbn = gbn.to_pgmpy()\n",
    "    if x_grad in gbn.isolated_nodes or e_grad in gbn.isolated_nodes:\n",
    "        return 0\n",
    "    inference = VariableElimination(pgmpy_gbn)\n",
    "    res_infer = inference.query([e_grad, x_grad])\n",
    "    dict_to_get_value = {e_grad:1, x_grad:1}\n",
    "    \n",
    "    return res_infer.get_value(**dict_to_get_value)\n",
    "\n",
    "\n",
    "def calculate_mutual_information(gbn, feat1, feat2, grad_map):\n",
    "\n",
    "\n",
    "    joint_values = {(x, y): calculate_cond_prob_support(gbn, x, y)\n",
    "                    for x, y in product(grad_map[feat1], grad_map[feat2])}\n",
    "    \n",
    "    all_sum = sum([x for x in joint_values.values()])\n",
    "\n",
    "    if all_sum == 0: # all gradations of feats are independent\n",
    "        return 0\n",
    "\n",
    "    joint_values = {k: v/all_sum for k, v in joint_values.items()}\n",
    "    marg_probs = {}\n",
    "    for grad in grad_map[feat1]:\n",
    "        s=sum([v for k, v in joint_values.items() if k[0]==grad])\n",
    "        marg_probs[grad]=s\n",
    "\n",
    "    for grad in grad_map[feat2]:\n",
    "        s=sum([v for k, v in joint_values.items() if k[1]==grad])\n",
    "        marg_probs[grad]=s\n",
    "\n",
    "    mutual_info = sum([joint_values[(x, y)]*np.log(joint_values[(x, y)]/(marg_probs[x]*marg_probs[y]) if marg_probs[x]*marg_probs[y]*joint_values[(x, y)] > 0 else 0\n",
    "                                                   ) for x, y in product(grad_map[feat1], grad_map[feat2])])\n",
    "\n",
    "    return mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_based_importance(data, error_col, feature, feat_to_grad):\n",
    "    error_probs = {grad: data[grad].mean() for grad in data.columns if error_col in grad}\n",
    "    s=0\n",
    "    for grad in feat_to_grad[feature]:\n",
    "        sub_sum = 0\n",
    "        for e, p in error_probs.items():\n",
    "            cprob = data[(data[e]==1)&(data[grad]==1)].shape[0]/data.shape[0]/data[grad].mean()\n",
    "            sub_sum+=cprob*np.log(cprob/p) if cprob>0 else 0\n",
    "        s+=data[grad].mean()*sub_sum\n",
    "    return s\n",
    "\n",
    "def mutual_information(data, error_col, feature, feat_to_grad):\n",
    "    s=0\n",
    "    for e_grad, f_grad in product([grad for grad in data.columns if error_col in grad], \n",
    "                                  feat_to_grad[feature]):\n",
    "        joint_prob = data[(data[e_grad]==1)&(data[f_grad]==1)].shape[0]/data.shape[0]\n",
    "        px, py = data[e_grad].mean(), data[f_grad].mean()\n",
    "\n",
    "        s+=joint_prob*np.log(joint_prob/px/py) if joint_prob>0 else 0\n",
    "\n",
    "    return s\n",
    "\n",
    "def error_based_importance_features(data, error_col, features, feat_to_grad):\n",
    "    df = pd.DataFrame(columns=['Feature', 'Metric value'])\n",
    "    for i, feature in enumerate(features):\n",
    "        val = error_based_importance(data, error_col, feature, feat_to_grad)\n",
    "        df.loc[i, :] = [feature, val]\n",
    "    return df\n",
    "\n",
    "def error_based_importance_features_bn(gbn, error_col, features, feat_to_grad):\n",
    "    df = pd.DataFrame(columns=['Feature', 'Metric value'])\n",
    "    for i, feature in tqdm(enumerate(features), total=len(features)):\n",
    "        val = calculate_mutual_information(gbn, error_col, feature, feat_to_grad)\n",
    "        df.loc[i, :] = [feature, val]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_classif(model, X, y):\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    return m.roc_auc_score(y, y_pred)\n",
    "\n",
    "def score_regr(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return -m.mean_absolute_percentage_error(y, y_pred) #negative mape\n",
    "\n",
    "def optunize_model(objective, features, target, data_dict, n_trials=30):\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)  \n",
    "    study.optimize(lambda trial: objective(trial, features, target, data_dict), n_trials=n_trials, show_progress_bar=True) #objective is implemented outside\n",
    "\n",
    "    return study.best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(model, data_dict, features_chosen, target, metric_dict, task='classif'):\n",
    "    model.fit(data_dict['train'][features_chosen], data_dict['train'][target])\n",
    "\n",
    "    if task == 'classif':\n",
    "        y_pred_test = model.predict_proba(data_dict['test'][features_chosen])[:, 1]\n",
    "    elif task == 'regr':\n",
    "        y_pred_test = model.predict(data_dict['test'][features_chosen])\n",
    "    metrics_vals = []\n",
    "    metrics_chng=[]\n",
    "    for m_name, metric in metric_dict.items():\n",
    "        val = metric(data_dict['test'][target], y_pred_test)\n",
    "        metrics_vals.append(np.round(val, 3))\n",
    "    return metrics_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimination_experiment(model, data_dict, metric_dict, importances, direction='asc', task='classif'):\n",
    "    ordered_features = importances.sort_values(by='Metric value', ascending=(direction=='asc'))\n",
    "    ordered_features = ordered_features['Feature'].tolist()\n",
    "    df_res = pd.DataFrame(columns=['n_features', 'last_excluded']+[m_name for m_name in metric_dict.keys()]\\\n",
    "                                                                +[m_name+'_change, %' for m_name in metric_dict.keys()])\n",
    "    for i in tqdm(range(len(ordered_features))):\n",
    "        model_c = clone(model)\n",
    "        features_chosen = ordered_features[i:]\n",
    "        # here we should choose new optimal parameters\n",
    "        \n",
    "        #model_c = model._class_\n",
    "\n",
    "        model_c.fit(data_dict['train'][features_chosen], data_dict['train'][target])\n",
    "\n",
    "        if task == 'classif':\n",
    "            y_pred_test = model_c.predict_proba(data_dict['test'][features_chosen])[:, 1]\n",
    "        elif task == 'regr':\n",
    "            y_pred_test = model_c.predict(data_dict['test'][features_chosen])\n",
    "        metrics_vals = []\n",
    "        metrics_chng=[]\n",
    "        for m_name, metric in metric_dict.items():\n",
    "            val = metric(data_dict['test'][target], y_pred_test)\n",
    "            metrics_vals.append(np.round(val, 3))\n",
    "            if i == 0:\n",
    "                metrics_chng.append(0)\n",
    "            else:\n",
    "                base = df_res.loc[0, m_name]\n",
    "                metrics_chng.append(np.round((val-base)/base*100, 3))\n",
    "        df_res.loc[i, :] = [len(features_chosen), ordered_features[i-1] if i > 0 else '-']+metrics_vals+metrics_chng\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimination_experiment_v2(model, data_dict, metric_dict, importances, objective, target, direction='asc', task='classif', n_trials=20):\n",
    "    ordered_features = importances.sort_values(by='Metric value', ascending=(direction=='asc'))\n",
    "    ordered_features = ordered_features['Feature'].tolist()\n",
    "    df_res = pd.DataFrame(columns=['n_features', 'last_excluded']+[m_name for m_name in metric_dict.keys()]\\\n",
    "                                                                +[m_name+'_change, %' for m_name in metric_dict.keys()])\n",
    "    for i in tqdm(range(len(ordered_features))):\n",
    "        model_c = clone(model)\n",
    "        features_chosen = ordered_features[i:]\n",
    "        # here we should choose new optimal parameters\n",
    "        opt_parameters = optunize_model(objective, features_chosen, target, data_dict, n_trials)\n",
    "        model_c = model.__class__(random_state=42, **opt_parameters)\n",
    "\n",
    "        model_c.fit(data_dict['train'][features_chosen], data_dict['train'][target])\n",
    "        if task == 'classif':\n",
    "            y_pred_test = model_c.predict_proba(data_dict['test'][features_chosen])[:, 1]\n",
    "        elif task == 'regr':\n",
    "            y_pred_test = model_c.predict(data_dict['test'][features_chosen])\n",
    "        \n",
    "\n",
    "        metrics_vals = []\n",
    "        metrics_chng=[]\n",
    "        for m_name, metric in metric_dict.items():\n",
    "            val = metric(data_dict['test'][target], y_pred_test)\n",
    "            metrics_vals.append(np.round(val, 3))\n",
    "            if i == 0:\n",
    "                metrics_chng.append(0)\n",
    "            else:\n",
    "                base = df_res.loc[0, m_name]\n",
    "                metrics_chng.append(np.round((val-base)/base*100, 3))\n",
    "        df_res.loc[i, :] = [len(features_chosen), ordered_features[i-1] if i > 0 else '-']+metrics_vals+metrics_chng\n",
    "\n",
    "        clear_output()\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_features(data, features_orig,extra_size=0.5, mode='random', random_seed=42):\n",
    "    extra_cnt = round(len(features_orig)/(1-extra_size))-len(features_orig)\n",
    "    data_c = data.copy(deep=True)\n",
    "    np.random.seed(random_seed)\n",
    "    if mode=='random':\n",
    "        for i in range(extra_cnt):\n",
    "            std = np.random.uniform(0.75, 1.25)\n",
    "            feat_name = f'extra_{str(i+1).zfill(3)}'\n",
    "            data_c[feat_name] = np.random.normal(scale=std, size=data_c.shape[0])\n",
    "    elif mode=='noisy':\n",
    "        for i in range(extra_cnt):\n",
    "            feature_chosen = np.random.choice(features_orig, size=1)[0]\n",
    "            std = data[feature_chosen].std()\n",
    "            feat_name = f'extra_{str(i+1).zfill(3)}'\n",
    "            data_c[feat_name] = 0.5*data[feature_chosen]+0.5*np.random.normal(scale=std, size=data_c.shape[0])\n",
    "    elif mode=='prod':\n",
    "        first_subset=np.random.choice(features_orig, size=extra_cnt)\n",
    "        second_subset=np.random.choice(features_orig, size=extra_cnt)\n",
    "        i=0\n",
    "        for feat1, feat2 in zip(first_subset, second_subset):\n",
    "            feat_name = f'extra_{str(i+1).zfill(3)}'\n",
    "            data_c[feat_name] = data[feat1]*data[feat2]\n",
    "            i+=1\n",
    "    return data_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_record(df, col='ROC AUC', mode='max'):\n",
    "    return df[df[col]==df[col].max()] if mode == 'max' else df[df[col]==df[col].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
