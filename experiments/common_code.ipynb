{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общий код между ноутбуками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дискретизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(data, variables, icat, icont, contdiscstrategy=\"kmeans\", n_bins=3):\n",
    "    transformers_data = dict()\n",
    "    \n",
    "    if icat is None: # в датасете только непрерывные фичи (и предиктор)\n",
    "        pipeline = make_pipeline(\n",
    "            KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", random_state=42, strategy=contdiscstrategy), \n",
    "            FunctionTransformer(lambda x: x.astype(\"int\")))\n",
    "    elif icont is None: # в датасете только категориальные фичи (и предиктор)\n",
    "        pipeline = make_pipeline(\n",
    "            OrdinalEncoder(categories=\"auto\"), \n",
    "            FunctionTransformer(lambda x: x.astype(\"int\")))\n",
    "    else:\n",
    "        pipeline = make_pipeline(make_union(\n",
    "        make_pipeline(FunctionTransformer(lambda x: x.iloc[:, icat]), OrdinalEncoder(categories=\"auto\")),\n",
    "        make_pipeline(FunctionTransformer(lambda x: x.loc[:, icont]), KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", random_state=42, strategy=contdiscstrategy))\n",
    "    ),\n",
    "        FunctionTransformer(lambda x: x.astype(\"int\")))\n",
    "    \n",
    "    \n",
    "    pipeline.fit(data)\n",
    "    encoded_data = pd.DataFrame(pipeline.transform(data), columns=data.columns if icat is None or icont is None\\\n",
    "                                                                                        else data.columns[icat+icont])\n",
    "    encoded_data = encoded_data[variables] # в общем случае пайплайн переставляет признаки, возвращаем их на их места здесь\n",
    "\n",
    "    for feat in encoded_data.columns:\n",
    "        if entropy(encoded_data[feat]) < 0.5:\n",
    "            print(f\"Warning: feature {feat} has practically degenerate states and low entropy\")\n",
    "    transformers_data[\"transformer\"] = pipeline\n",
    "    \n",
    "    if icat is None:\n",
    "        transformers_data[\"cont_features\"]       = list(pipeline.steps[0][1].get_feature_names_out())\n",
    "        transformers_data[\"cont_features_edges\"] = pipeline.steps[0][1].bin_edges_\n",
    "        \n",
    "    elif icont is None:\n",
    "        transformers_data[\"cat_features\"]            = list(pipeline.steps[0][1].get_feature_names_out())\n",
    "        transformers_data[\"cat_features_categories\"] = pipeline.steps[0][1].categories_\n",
    "        \n",
    "    else:\n",
    "        transformers_data[\"cont_features\"]           = list(pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].get_feature_names_out())\n",
    "        transformers_data[\"cont_features_edges\"]     = pipeline.steps[0][1].named_transformers[\"pipeline-2\"].steps[1][1].bin_edges_\n",
    "        transformers_data[\"cat_features\"]            = list(pipeline.steps[0][1].named_transformers[\"pipeline-1\"].steps[1][1].get_feature_names_out())\n",
    "        transformers_data[\"cat_features_categories\"] = pipeline.steps[0][1].named_transformers[\"pipeline-1\"].steps[1][1].categories_\n",
    "    \n",
    "    #kmeanspipeline.steps[0][1].named_transformers[\"pipeline-1\"].steps[1][1].categories_[0]\n",
    "    return encoded_data, transformers_data\n",
    "\n",
    "#disc_data, pipeline_data = discretize(data, variables, icat=None, icont=[0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка таблиц усл. вероятностей по БС с градациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_cpds(bn_info, distributions, n_states_map):\n",
    "    cpds = list()\n",
    "    for index, row in bn_info.iterrows():\n",
    "        feat = row[\"name\"].name\n",
    "        if len(row[\"parents\"]) == 0:\n",
    "            # cpd is just a pd\n",
    "            cpd = TabularCPD(feat, n_states_map[feat], [[e] for e in distributions[feat][\"cprob\"]])\n",
    "            cpds.append(cpd)\n",
    "        else:\n",
    "            cpd_list = [probs for probs in distributions[feat][\"cprob\"].values()]\n",
    "            #cpd_list = [probs for i, probs in distributions[feat][\"cprob\"].items() if i[0]!=\"[\"]\n",
    "            #print(cpd_list)\n",
    "            nrows = len(cpd_list)\n",
    "            ncols = len(cpd_list[0])\n",
    "            cpd_list = [[cpd_list[i][j] for i in range(nrows)] for j in range(ncols)]\n",
    "            #print(feat, row[\"parents\"])\n",
    "            #print(cpd_list)\n",
    "            cpd = TabularCPD(feat, n_states_map[feat], cpd_list, evidence=row[\"parents\"], evidence_card=[n_states_map[p] for p in row[\"parents\"]])\n",
    "            cpds.append(cpd)\n",
    "    return cpds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(data,\n",
    "                       d_dict,# словарь списков фактор-градация\n",
    "                       b_sample_size,\n",
    "                       metrics_list, trials=1000, alpha=0, incl_random_removal=False,\n",
    "                       mode='regr',\n",
    "                       drop_mode='random'):\n",
    "    \n",
    "    n_random=0\n",
    "\n",
    "    metrics_results={\n",
    "        k: [list() for _ in range(len(metrics_list))] for k in d_dict.keys()\n",
    "    }\n",
    "    metrics_results['init'] = [list() for _ in range(len(metrics_list))]\n",
    "\n",
    "    if incl_random_removal:\n",
    "        metrics_results['random'] = [list() for _ in range(len(metrics_list))]\n",
    "\n",
    "\n",
    "    n_dropped_stats={k: list() for k in metrics_results.keys()}\n",
    "\n",
    "    #пробуем побутсрапировать выборку, чтобы оценить значимость различий в ошибках регрессии\n",
    "    np.random.seed(42)\n",
    "    for i in tqdm(range(trials)):\n",
    "        indexes = np.random.choice(data.index, size=b_sample_size)\n",
    "        bsample = data.loc[indexes]\n",
    "        \n",
    "        model_base = copy(model)\n",
    "        model_base.fit(bsample[features], bsample[target])\n",
    "\n",
    "        y_pred = model_base.predict(bsample[features]) # depends on task (regr/classif)\n",
    "        \n",
    "        data_errors = bsample.copy(deep=True)\n",
    "        data_errors[\"ape_error\"] = np.abs((bsample[target]-y_pred)/bsample[target])\n",
    "        data_errors.drop(columns=[target], inplace=True)\n",
    "\n",
    "        bsample_disc = pd.DataFrame(pipeline_data[\"transformer\"].transform(data_errors.loc[indexes]), columns=data_errors.columns)\n",
    "        \n",
    "        mask = False\n",
    "\n",
    "        for g in metrics_results.keys():\n",
    "            \n",
    "            if g == 'init':\n",
    "                mask = None\n",
    "                samp = bsample\n",
    "            elif g == 'random':\n",
    "                mask=None\n",
    "                indexes_to_drop = np.random.choice(bsample.index, size=n_random, replace=False)\n",
    "                samp = samp = bsample.drop(index=indexes_to_drop)\n",
    "            else:\n",
    "                mask=False\n",
    "                for feat, cat in zip(d_dict[g][0], d_dict[g][1]):\n",
    "                    mask = mask | (bsample_disc[feat]==cat)\n",
    "                \n",
    "                if drop_mode == 'random':\n",
    "                    indexes_to_drop = np.random.choice(bsample[mask].index, size=int(np.floor(bsample[mask].shape[0]*(1-alpha))), replace=False)\n",
    "                elif drop_mode == 'metric':\n",
    "                    errors = data_errors[mask]['ape_error']\n",
    "                    errors_sorted = errors.sort_values(ascending=False)\n",
    "                    size = int(np.floor(errors.shape[0]*(1-alpha)))\n",
    "                    thresh = errors_sorted[:size].min()\n",
    "                    indexes_to_drop = errors[errors>=thresh].index\n",
    "\n",
    "                \n",
    "                samp = bsample.drop(index=indexes_to_drop)\n",
    "                if g=='bn':\n",
    "                    n_random=indexes_to_drop.shape[0]\n",
    "                    \n",
    "                n_dropped_stats[g].append(indexes_to_drop.shape[0])\n",
    "        # --\n",
    "            \n",
    "            X1, y1 = samp[features], samp[target]\n",
    "\n",
    "            model1 = copy(model)\n",
    "            model1.fit(X1, y1)  \n",
    "\n",
    "            y1_pred = model1.predict(X1)\n",
    "\n",
    "            for k, metric in enumerate(metrics_list):\n",
    "                metrics_results[g][k].append(metric(y1, y1_pred))\n",
    "        \n",
    "    return {'metrics': metrics_results,\n",
    "             'n_dropped': n_dropped_stats,\n",
    "             'avg_n_dropped': {k: sum(v)/len(v) if len(v) >0 else -1 for k, v in n_dropped_stats.items()}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
